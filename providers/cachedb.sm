#! ruby

##  CacheDB, a transactional minimalism-oriented JSON "data base" implementation
##    Copyright Cat Stevens 2018
##  This program is free software: you can redistribute it and/or modify
##    it under the terms of the GNU General Public License as published by
##    the Free Software Foundation, either version 3 of the License, or
##    (at your option) any later version.
##
##    This program is distributed in the hope that it will be useful,
##    but WITHOUT ANY WARRANTY; without even the implied warranty of
##    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
##    GNU General Public License for more details.
##
##    You should have received a copy of the GNU General Public License
##    along with this program.  If not, see <https://www.gnu.org/licenses/>.

include sidefext::construct
include sidefext::iterable

include lib::guardio
include lib::feature
include lib::withdirectory
include lib::ABCs

import sidefext::iterable::ImmutableHash

module CacheDBIO {
  import sidefext::iterable::ImmutableHash

  include Atomic

  define JSON = Feature::JSON.new

  # look for the database file
  func _read_disk_db (File name, lib::ABCs::Serializable element_type) is cached -> ImmutableHash {

    # look for the database file
    # if it doesn't exist, stop
    # otherwise, open it
    if ( Atomic::aread(name) ) {
        |contents|
      ImmutableHash( Feature::JSON.decode(contents) ) \
        # TODO: fixup this use of unserialize on Hash<Serializable>
        .map_kv{
            |_, v|
          ( (_) =>
            ( sidefext::construct::maybe_init( element_type ) ) -> unserialize(v) )
        }
    } else {
      # if it's empty, get a new one
      ImmutableHash()
    }

  }

  func _write_disk_db (File name, ImmutableHash new_data) {
    # look for the database file
    # if it doesn't exist, stop
    # otherwise, open it
    # fixup this use of .serialize on a Hash<Serializable>
    const h = JSON.pretty.encode( new_data.map_kv{ |_, v| ((_) => v.serialize) } )
    say h
    Atomic::awrite( name, h )
  }
}

module DBTransform {
  import sidefext::iterable::ImmutableHash
  # const ImmHash = ImmutableHash

  define Regex ACTION_IS_BY = /_by$/
  define Regex ACTION_NOT_BY = /(?!_by).{3}$/

  define Regex ACTION_IS_CHANGE = /^change/
  define Regex ACTION_IS_INSERT = /^insert/
  define Regex ACTION_IS_DELETE = /^delete/

  define Set ACTIONS = Set(:insert, :insert_by, :change, :change_by, :delete, :delete_by)

  define _a = assert_eq( ACTIONS.grep{ .~~ ACTION_IS_BY}.len, ACTIONS.grep{ .~~ ACTION_NOT_BY}.len )

  subset Action < String { ACTIONS.has(_) }

  # Good afternoon, gentleman, I'm a C compiler
  struct Transformation {
    action < Action,
    String key,
    Block action_by,
    object,
    String uuid,
    Time generated_at
  }

  func _chk (Transformation _ { .action ~~ ACTION_IS_BY }) is cached -> Bool {
    !defined(.object) && defined(.action_by)
  }
  func _chk (Transformation _ { .action ~~ ACTION_NOT_BY }) is cached -> Bool {
    defined(.object) && .action_by.refaddr==Block.list_identity.refaddr
  }

  subset Change   < Transformation { .action ~~ ACTION_IS_CHANGE && _chk(_) }
  subset ChangeBy < Transformation { .action ~~ ACTION_IS_CHANGE && _chk(_) }
  subset Insert   < Transformation { .action ~~ ACTION_IS_INSERT && _chk(_) }
  subset InsertBy < Transformation { .action ~~ ACTION_IS_INSERT && _chk(_) }
  subset Delete   < Transformation { .action ~~ ACTION_IS_DELETE && _chk(_) }
  subset DeleteBy < Transformation { .action ~~ ACTION_IS_DELETE && _chk(_) }

  func new ( a < Action { .~~ ACTION_IS_BY }, Str k, Block b) -> Transformation {
    Transformation( a, k, b, nil, GuardIO::make_uuid(len: 10), Time() )
  }
  # negative lookahead with EOS anchor https://stackoverflow.com/questions/31683167/regex-with-anchor-and-look-ahead
  func new ( a < Action { .~~ ACTION_NOT_BY }, Str k, obj) -> Transformation {
    Transformation( a, k, Block.list_identity, obj, GuardIO::make_uuid(len: 10), Time() )
  }

  -> apply (ImmutableHash _, tf < Insert)   -> ImmutableHash { ImmutableHash(. + :(tf.key tf.object)) }
  -> apply (ImmutableHash _, tf < InsertBy) -> ImmutableHash { ImmutableHash(. + :(tf.key tf.action_by.run(tf.key, .item(tf.key)))) }
  -> apply (ImmutableHash _, tf < Change)   -> ImmutableHash { ImmutableHash(.-tf.key + :(tf.key tf.object)) }
  -> apply (ImmutableHash _, tf < ChangeBy) -> ImmutableHash { ImmutableHash(.-tf.key + :(tf.key tf.action_by.run(tf.key, .item(tf.key)))) }
  -> apply (ImmutableHash _, tf < Delete)   -> ImmutableHash { ImmutableHash(.  - tf.key) }
  -> apply (ImmutableHash _, tf < DeleteBy) -> ImmutableHash { ImmutableHash(.grep_kv{ |k, v| ! tf.action_by.run(k, v) }) }

  func apply ( ImmutableHash data, Set tfs ) -> ImmutableHash {
    tfs.values.reduce( apply, data )
  }
}

define Hash TEMPLATE_SETUP_CACHEDB = :(
  :dir => Dir,
  :filename_base => File,
  :dry_run => Bool,
  :verbose => Bool,
  :trace => Bool,
  :replace_existing => Bool,
  :debug => :( :type => Bool, :default => true ),
  :logger => :( :type => GuardIO::SLog, :deferred => { GuardIO::SLog( is_verbose: .{:verbose}, is_trace: .{:trace} ) } ),
  :element_type => :( :type => lib::ABCs::Serializable, :val_is_typename => true )
)

class CacheDB < lib::withdirectory::CapturedWriter {

  has Set change_sequence = Set()
  has ImmutableHash orig_data = ImmutableHash()
  has GuardIO::SLog log
  has Bool _write_called
  has Bool _read_called

  has Bool dry_run
  has Dir db_dir
  has File filename_base
  has lib::ABCs::Serializable element_type

  -> db_name () is cached -> File { self.db_dir + self.filename_base }
  # for CapturedWriter
  -> wd () is cached -> Dir { self.db_dir }

  -> data () is cached -> ImmutableHash { self.orig_data }

  method setup (Hash cdb_opts) -> __CLASS__ {
    TEMPLATE_SETUP_CACHEDB.template_transform(cdb_opts).rescope{
      self.log = (*.{:logger})(_)
      self.dry_run = .{:dry_run}

      self.db_dir = .{:dir}
      self.filename_base = .{:filename_base}

      self.element_type = .{:element_type}

      if (.{:replace_existing} && self.db_name.exists) {
        self.log.wrn(__FILE__, __LINE__, __METHOD_NAME__, "!!! removing old DB !!!")
        self.db_name.cleave({ .remove }, { .touch })
      } elsif ( ! self.db_name.exists ) {
        self.db_name.touch
      }
    }

    [:change, :insert, :delete].each{
        |action|
      const action_by = (action + :_by)

      func generated_action (__CLASS__ self, String k, Object o) -> __CLASS__ {
        # self.log.wrn(__FILE__, __LINE__, __FUNC_NAME__, ab, o.id, o.dump.first(50))
        self.transform( DBTransform::new(action, k, o) )
      }
      func generated_action (__CLASS__ self, Object o) -> __CLASS__ {
        self.transform( DBTransform::new(action, o.id, o) )
      }

      func generated_action_by (__CLASS__ self, String k, Block b) -> __CLASS__ {
        self.transform( DBTransform::new(action_by, k, b) )
      }

      __CLASS__.def_method( action, generated_action )
      __CLASS__.def_method( action_by, generated_action_by )
    }

    self.orig_data = self._read_disk_db
    self
  }

  method _write_disk_db {
    if ( self._write_called ) { die "bug found: you can't call #{__METHOD_NAME__} more than once" }
    self._write_called = true

    self.log.trc(__FILE__, __LINE__, __METHOD_NAME__, 'writing', self.db_name)

    self.capture_write_disk(
      __METHOD_NAME__,
      { CacheDBIO::_write_disk_db( self.filename_base, DBTransform::apply(self.orig_data, self.change_sequence) ) }
    )
  }

  method _read_disk_db () is cached -> ImmutableHash {
    if ( self._read_called) { die "bug found: you can't call #{__METHOD_NAME__} more than once" }
    self._read_called = true
    self.log.trc(__FILE__, __LINE__, __METHOD_NAME__, 'reading', self.filename_base)

    self.with_directory{ CacheDBIO::_read_disk_db(self.filename_base, self.element_type) }
  }

  method destroy (Bool is_exc, Hash _exc_info) {
    self.log.wrn(__FILE__, __LINE__, __METHOD_NAME__, 'shutting down!!!')
    ( is_exc
      ? self.log.wrn(__FILE__, __LINE__, __METHOD_NAME__, 'not writing disk because of an exception')
      : self._write_disk_db )
  }

  method transform (DBTransform::Transformation tf) -> __CLASS__ {
    self.change_sequence.append(tf)
    self
  }
  method transform (Set tfs) -> __CLASS__ {
    self.change_sequence.concat(tfs)
    self
  }
}
